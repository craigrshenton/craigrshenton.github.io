---
layout: post
title: "MS Azure with python- 1.0 feature reduction"
tags:
    - python
    - notebook
---

Following 'Fundamentals of Data Science with Python' course Microsoft's new [Azure Notebooks](https://notebooks.azure.com/) site.

# Introduction to Machine Learning with `scikit-learn I`

### Preprocessing i.e., make the learning easier or better  beforehand -  feature reduction/selection/creation
* SelectKBest
* PCA

### Selecting k top scoring features (also dimensionality reduction)

**In [1]:**

```python
%matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

**In [2]:**

```python
# SelectKBest for selecting top-scoring features

from sklearn import datasets
from sklearn.feature_selection import SelectKBest, chi2

iris = datasets.load_iris()
X, y = iris.data, iris.target
df = pd.DataFrame(X, columns = iris.feature_names)
print(X.shape)
df.head()
```

    (150, 4)
    

<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>
</div>



**In [3]:**


```python
# New feature is petal width / sepal width
df['petal width / sepal width'] = df['petal width (cm)'] / df['sepal width (cm)']

# Grab feature names + new one
new_feature_names = df.columns
print('New feature names:', list(new_feature_names))

# We've now added a new column to our data
X = np.array(df)
```

    New feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)', 'petal width / sepal width']
    

**In [4]:**

```python
# Perform feature selection

#  Input is scoring function (here chi2) to get univariate p-values
#  and number of top-scoring features (k) - here we get the top 3
dim_red = SelectKBest(chi2, k = 3)

# Train
dim_red.fit(X, y)

# Use model to transform original data
X_t = dim_red.transform(X)
```

**In [5]:**

```python
# Show scores, features selected and new shape
print('Scores:', dim_red.scores_)
print('New shape:', X_t.shape)
```

    Scores: [  10.81782088    3.59449902  116.16984746   67.24482759]
    New shape: (150, 3)
    

**In [6]:**

```python
# Get back the selected columns
selected = dim_red.get_support()
selected_names = new_feature_names[selected]

print('Top k features: ', list(selected_names))
```

    Top k features:  ['petal length (cm)', 'petal width (cm)', 'petal width / sepal width']
    

**Note on scoring function selection in `SelectKBest` tranformations:**
* For regression - [f_regression](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression)
* For classification - [chi2](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2), [f_classif](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif)


### Principal component analysis (aka PCA)
* Reduces dimensions (number of features), based on what information explains the most variance (or signal)
* Considered unsupervised learning
* Useful for very large feature space (e.g. say the botanist in charge of the iris dataset measured 100 more parts of the flower and thus there were 104 columns instead of 4)

**In [7]:**

```python
# PCA for dimensionality reduction

from sklearn import decomposition
from sklearn import datasets

iris = datasets.load_iris()

X, y = iris.data, iris.target

# Perform principal component analysis
# Selects the number of components such that the amount of variance 
#   that needs to be explained is greater than the percentage specified
pca = decomposition.PCA(0.95)
pca.fit(X)

# Apply pca to data - like SelectKBest above
X_t = pca.transform(X)

# Check the dimensions of the transformed data in X_t
X_t.shape
```




    (150, 2)



**In [8]:**

```python
# Grab the first two principle components
x1, x2 = X_t[:, 0], X_t[:, 1]

#  (note: you can get the iris names below from iris.target_names, also in docs)
c1 = np.array(list('rbg')) # colors
colors = c1[y] # y coded by color
classes = iris.target_names[y] # y coded by iris name
for (i, cla) in enumerate(set(classes)):
    xc = [p for (j, p) in enumerate(x1) if classes[j] == cla]
    yc = [p for (j, p) in enumerate(x2) if classes[j] == cla]
    cols = [c for (j, c) in enumerate(colors) if classes[j] == cla]
    plt.scatter(xc, yc, c = cols, label = cla)
    plt.ylabel('Principal Component 2')
    plt.xlabel('Principal Component 1')
plt.legend(loc = 4)
```




    <matplotlib.legend.Legend at 0x251a9e9aeb8>




![png]({{ site.baseurl }}/notebooks/notebook9_files/notebook9_70_1.png)
