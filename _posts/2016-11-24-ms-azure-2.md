---
layout: post
title: "MS Azure- 1.1 principal component analysis"
tags:
    - python
    - notebook
---

Following 'Fundamentals of Data Science with Python' course @ Microsoft's new [Azure Notebooks](https://notebooks.azure.com/) site

Principal component analysis (aka PCA)
* Reduces dimensions (number of features), based on what information explains the most variance (or signal)
* Considered unsupervised learning
* Useful for very large feature space

**In [1]:**

```python
# PCA for dimensionality reduction

from sklearn import decomposition
from sklearn import datasets

iris = datasets.load_iris()

X, y = iris.data, iris.target

# Perform principal component analysis
# Selects the number of components such that the amount of variance 
#   that needs to be explained is greater than the percentage specified
pca = decomposition.PCA(0.95)
pca.fit(X)

# Apply pca to data - like SelectKBest above
X_t = pca.transform(X)

# Check the dimensions of the transformed data in X_t
X_t.shape
```

    (150, 2)

**In [2]:**

```python
# Grab the first two principle components
x1, x2 = X_t[:, 0], X_t[:, 1]

#  (note: you can get the iris names below from iris.target_names, also in docs)
c1 = np.array(list('rbg')) # colors
colors = c1[y] # y coded by color
classes = iris.target_names[y] # y coded by iris name
for (i, cla) in enumerate(set(classes)):
    xc = [p for (j, p) in enumerate(x1) if classes[j] == cla]
    yc = [p for (j, p) in enumerate(x2) if classes[j] == cla]
    cols = [c for (j, c) in enumerate(colors) if classes[j] == cla]
    plt.scatter(xc, yc, c = cols, label = cla)
    plt.ylabel('Principal Component 2')
    plt.xlabel('Principal Component 1')
plt.legend(loc = 4)
```

    <matplotlib.legend.Legend at 0x251a9e9aeb8>


![png]({{ site.baseurl }}/notebooks/notebook9_files/notebook9_70_1.png)
