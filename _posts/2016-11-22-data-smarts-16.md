---
layout: post
title: "Data smarts in python- 5.2 logistic regression (revisited)"
tags:
    - python
    - notebook
---

Continuing from chapter 5. Following [General Assembly's Data Science course](https://generalassemb.ly/education/data-science/washington-dc/), logistic regression with 'sklearn' and cross-validation

**In [1]:**

```python
# define X and y
X = data[train_cols]
y = df_accounts['Pregnant']

# train/test split
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# train a logistic regression model
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(C=1e9)
logreg.fit(X_train, y_train)

# make predictions for testing set
y_pred_class = logreg.predict(X_test)

# calculate testing accuracy
from sklearn import metrics
print(metrics.accuracy_score(y_test, y_pred_class))
```

    0.88
    

**In [2]:**

```python
# predict probability of survival
y_pred_prob = logreg.predict_proba(X_test)[:, 1]

import matplotlib.pyplot as plt

# plot ROC curve
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)
plt.plot(fpr, tpr)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.show()
```


![png]({{ site.baseurl }}/notebooks/notebook8_files/notebook8_14_0.png)


**In [3]:**

```python
# calculate AUC
print(metrics.roc_auc_score(y_test, y_pred_prob))
```

    0.94394259722
    

**In [4]:**

```python
# histogram of predicted probabilities grouped by actual response value
df = pd.DataFrame({'probability':y_pred_prob, 'actual':y_test})
df.hist(column='probability', by='actual', sharex=True, sharey=True)
plt.show()
```


![png]({{ site.baseurl }}/notebooks/notebook8_files/notebook8_16_0.png)


**In [5]:**

```python
# calculate cross-validated AUC
from sklearn.cross_validation import cross_val_score
cross_val_score(logreg, X, y, cv=10, scoring='roc_auc').mean()
```




    0.89871999999999996



